{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Lowercase, replace non-alnum with spaces, collapse whitespace, strip.\"\"\"\n",
    "    if s is None: \n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^0-9a-z\\u00C0-\\u017F]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return normalize(s).split() if s else []\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, gold: Dict[str, List[str]], pred: Dict[str, List[str]]):\n",
    "        \"\"\"\n",
    "        gold, pred: dict mapping doc_id -> list of term strings\n",
    "        \"\"\"\n",
    "        self.gold = {d: list(gold.get(d, [])) for d in gold}\n",
    "        self.pred = {d: list(pred.get(d, [])) for d in pred}\n",
    "        for d in set(list(self.gold.keys()) + list(self.pred.keys())):\n",
    "            self.gold.setdefault(d, [])\n",
    "            self.pred.setdefault(d, [])\n",
    "\n",
    "    # ---- Exact-match metrics ----\n",
    "    def exact(self) -> Tuple[float,float,float, Dict]:\n",
    "        tp = fp = fn = 0\n",
    "        details = {}\n",
    "        for d in sorted(self.gold.keys()):\n",
    "            gset = Counter([normalize(t) for t in self.gold[d]])\n",
    "            pset = Counter([normalize(t) for t in self.pred[d]])\n",
    "            doc_tp = sum(min(gset[t], pset[t]) for t in set(gset) | set(pset))\n",
    "            doc_fp = sum(pset.values()) - doc_tp\n",
    "            doc_fn = sum(gset.values()) - doc_tp\n",
    "            tp += doc_tp; fp += doc_fp; fn += doc_fn\n",
    "            details[d] = {'tp':doc_tp,'fp':doc_fp,'fn':doc_fn,\n",
    "                          'gold':sum(gset.values()), 'pred':sum(pset.values())}\n",
    "        p = tp/(tp+fp) if (tp+fp) else 0\n",
    "        r = tp/(tp+fn) if (tp+fn) else 0\n",
    "        f1 = (2*p*r/(p+r)) if (p+r) else 0\n",
    "        return p,r,f1,details\n",
    "\n",
    "    # ---- Substring match ----\n",
    "    def substring(self) -> Tuple[float,float,float, Dict]:\n",
    "        tp = fp = fn = 0\n",
    "        details = {}\n",
    "        for d in sorted(self.gold.keys()):\n",
    "            gold_list = [normalize(t) for t in self.gold[d]]\n",
    "            pred_list = [normalize(t) for t in self.pred[d]]\n",
    "            used_gold = [False]*len(gold_list)\n",
    "            doc_tp = 0\n",
    "            for p in pred_list:\n",
    "                matched = False\n",
    "                for i,g in enumerate(gold_list):\n",
    "                    if used_gold[i]: \n",
    "                        continue\n",
    "                    if p == g or (' '+p+' ') in (' '+g+' ') or (' '+g+' ') in (' '+p+' '):\n",
    "                        matched = True\n",
    "                        used_gold[i] = True\n",
    "                        break\n",
    "                    if p in g or g in p:\n",
    "                        matched = True\n",
    "                        used_gold[i] = True\n",
    "                        break\n",
    "                if matched:\n",
    "                    doc_tp += 1\n",
    "            doc_fp = len(pred_list) - doc_tp\n",
    "            doc_fn = len(gold_list) - doc_tp\n",
    "            tp += doc_tp; fp += doc_fp; fn += doc_fn\n",
    "            details[d] = {'tp':doc_tp,'fp':doc_fp,'fn':doc_fn,\n",
    "                          'gold':len(gold_list), 'pred':len(pred_list)}\n",
    "        p = tp/(tp+fp) if (tp+fp) else 0\n",
    "        r = tp/(tp+fn) if (tp+fn) else 0\n",
    "        f1 = (2*p*r/(p+r)) if (p+r) else 0\n",
    "        return p,r,f1,details\n",
    "\n",
    "    # ---- IoU token-overlap ----\n",
    "    def iou_match(self, iou_threshold=0.5) -> Tuple[float,float,float, Dict]:\n",
    "        tp = fp = fn = 0\n",
    "        details = {}\n",
    "        for d in sorted(self.gold.keys()):\n",
    "            gold_tokens = [set(tokenize(g)) for g in self.gold[d]]\n",
    "            pred_tokens = [set(tokenize(p)) for p in self.pred[d]]\n",
    "            used_gold = [False]*len(gold_tokens)\n",
    "            doc_tp = 0\n",
    "            for p_tok in pred_tokens:\n",
    "                best_i, best_iou = None, 0\n",
    "                for i,g_tok in enumerate(gold_tokens):\n",
    "                    if used_gold[i]:\n",
    "                        continue\n",
    "                    inter = len(p_tok & g_tok)\n",
    "                    union = len(p_tok | g_tok)\n",
    "                    iou = inter/union if union else 0\n",
    "                    if iou > best_iou:\n",
    "                        best_iou, best_i = iou, i\n",
    "                if best_i is not None and best_iou >= iou_threshold:\n",
    "                    doc_tp += 1\n",
    "                    used_gold[best_i] = True\n",
    "            doc_fp = len(pred_tokens) - doc_tp\n",
    "            doc_fn = len(gold_tokens) - doc_tp\n",
    "            tp += doc_tp; fp += doc_fp; fn += doc_fn\n",
    "            details[d] = {'tp':doc_tp,'fp':doc_fp,'fn':doc_fn,\n",
    "                          'gold':len(gold_tokens), 'pred':len(pred_tokens),\n",
    "                          'iou_threshold': iou_threshold}\n",
    "        p = tp/(tp+fp) if (tp+fp) else 0\n",
    "        r = tp/(tp+fn) if (tp+fn) else 0\n",
    "        f1 = (2*p*r/(p+r)) if (p+r) else 0\n",
    "        return p,r,f1,details\n",
    "\n",
    "    # ---- Token-level metrics ----\n",
    "    def token_level(self) -> Tuple[float,float,float, Dict]:\n",
    "        tp = fp = fn = 0\n",
    "        details = {}\n",
    "        for d in sorted(self.gold.keys()):\n",
    "            gcount = Counter([tok for g in self.gold[d] for tok in tokenize(g)])\n",
    "            pcount = Counter([tok for p in self.pred[d] for tok in tokenize(p)])\n",
    "            doc_tp = sum(min(gcount[t], pcount[t]) for t in set(gcount) | set(pcount))\n",
    "            doc_fp = sum(pcount.values()) - doc_tp\n",
    "            doc_fn = sum(gcount.values()) - doc_tp\n",
    "            tp += doc_tp; fp += doc_fp; fn += doc_fn\n",
    "            details[d] = {'tp':doc_tp,'fp':doc_fp,'fn':doc_fn,\n",
    "                          'gold_tokens':sum(gcount.values()),\n",
    "                          'pred_tokens':sum(pcount.values())}\n",
    "        p = tp/(tp+fp) if (tp+fp) else 0\n",
    "        r = tp/(tp+fn) if (tp+fn) else 0\n",
    "        f1 = (2*p*r/(p+r)) if (p+r) else 0\n",
    "        return p,r,f1,details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/oarga/saccrow-data/papers/benchmark_papers_all'\n",
    "\n",
    "docs_pred = {}\n",
    "docs_gold = {}\n",
    "texts = {}\n",
    "\n",
    "# list all folders in path\n",
    "import os\n",
    "import csv\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    if not os.path.isdir(os.path.join(path, f)):\n",
    "        continue\n",
    "    pred_file = 'paper/plain/processed/abstract.csv'\n",
    "    gold_file = 'paper/plain/processed/gt_abstract.txt'\n",
    "    text_file = 'paper/plain/processed/abstract.txt'\n",
    "    pred_path = os.path.join(path, f, pred_file)\n",
    "    gold_path = os.path.join(path, f, gold_file)\n",
    "    text_path = os.path.join(path, f, text_file)\n",
    "    # read each row of txt to list\n",
    "    with open(gold_path, 'r') as inf:\n",
    "        docs_gold[f] = [line.strip() for line in inf if line.strip()]\n",
    "    # read first col of each row as csv\n",
    "    with open(pred_path, 'r') as inf:\n",
    "        reader = csv.reader(inf, delimiter=',')\n",
    "        docs_pred[f] = [row[0].strip() for row in reader if row and row[0].strip()]    \n",
    "    with open(text_path, 'r') as inf:\n",
    "        texts[f] = inf.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_results = {}\n",
    "substring_results = {}\n",
    "iou_results = {}\n",
    "token_results = {}\n",
    "\n",
    "for dd in [exact_results, substring_results, iou_results, token_results]:\n",
    "    dd['p_list'] = []\n",
    "    dd['r_list'] = []\n",
    "    dd['f1_list'] = []\n",
    "    dd['tp_list'] = []\n",
    "    dd['fp_list'] = []\n",
    "    dd['fn_list'] = []\n",
    "    dd['gold_list'] = []\n",
    "    dd['pred_list'] = []\n",
    "\n",
    "def add_results(ddict, results_item):\n",
    "    p, r, f1, details = results_item\n",
    "    ddict['p_list'].append(p)\n",
    "    ddict['r_list'].append(r)\n",
    "    ddict['f1_list'].append(f1)\n",
    "    for d in details.values():\n",
    "        ddict['tp_list'].append(d['tp'])\n",
    "        ddict['fp_list'].append(d['fp'])\n",
    "        ddict['fn_list'].append(d['fn'])\n",
    "        ddict['gold_list'].append(d.get('gold', d.get('gold_tokens', 0)))\n",
    "        ddict['pred_list'].append(d.get('pred', d.get('pred_tokens', 0)))\n",
    "\n",
    "for k in docs_gold.keys():\n",
    "    print(k)\n",
    "    gold = {\n",
    "        k: docs_gold[k]\n",
    "    }\n",
    "    pred = {\n",
    "        k: docs_pred[k]\n",
    "    }\n",
    "\n",
    "    ev = Evaluator(gold, pred)\n",
    "    ee = ev.exact()\n",
    "    print(\"    === Exact-match ===\", ee)\n",
    "    add_results(exact_results, ee)\n",
    "\n",
    "    ee = ev.substring()\n",
    "    print(\"    === Substring-match ===\", )\n",
    "    add_results(substring_results, ee)\n",
    "\n",
    "    ee = ev.iou_match(0.5)\n",
    "    print(\"    === IoU token-overlap (0.5) ===\", ee)\n",
    "    add_results(iou_results, ee)\n",
    "\n",
    "    ee = ev.token_level()\n",
    "    print(\"    === Token-level ===\", ee)\n",
    "    add_results(token_results, ee)\n",
    "    \n",
    "    print()\n",
    "\n",
    "for k, v in exact_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Exact-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in substring_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Substring-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in iou_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"IoU-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in token_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Token-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/oarga/saccrow-data/papers/benchmark_papers_all'\n",
    "\n",
    "docs_pred = {}\n",
    "docs_gold = {}\n",
    "texts = {}\n",
    "\n",
    "# list all folders in path\n",
    "import os\n",
    "import csv\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    if not os.path.isdir(os.path.join(path, f)):\n",
    "        continue\n",
    "    pred_file = 'paper/plain/processed/abstract_nosplit.csv'\n",
    "    gold_file = 'paper/plain/processed/gt_abstract.txt'\n",
    "    text_file = 'paper/plain/processed/abstract.txt'\n",
    "    pred_path = os.path.join(path, f, pred_file)\n",
    "    gold_path = os.path.join(path, f, gold_file)\n",
    "    text_path = os.path.join(path, f, text_file)\n",
    "    # read each row of txt to list\n",
    "    with open(gold_path, 'r') as inf:\n",
    "        docs_gold[f] = [line.strip() for line in inf if line.strip()]\n",
    "    # read first col of each row as csv\n",
    "    with open(pred_path, 'r') as inf:\n",
    "        reader = csv.reader(inf, delimiter=',')\n",
    "        docs_pred[f] = [row[0].strip() for row in reader if row and row[0].strip()]    \n",
    "    with open(text_path, 'r') as inf:\n",
    "        texts[f] = inf.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_results = {}\n",
    "substring_results = {}\n",
    "iou_results = {}\n",
    "token_results = {}\n",
    "\n",
    "for dd in [exact_results, substring_results, iou_results, token_results]:\n",
    "    dd['p_list'] = []\n",
    "    dd['r_list'] = []\n",
    "    dd['f1_list'] = []\n",
    "    dd['tp_list'] = []\n",
    "    dd['fp_list'] = []\n",
    "    dd['fn_list'] = []\n",
    "    dd['gold_list'] = []\n",
    "    dd['pred_list'] = []\n",
    "\n",
    "def add_results(ddict, results_item):\n",
    "    p, r, f1, details = results_item\n",
    "    ddict['p_list'].append(p)\n",
    "    ddict['r_list'].append(r)\n",
    "    ddict['f1_list'].append(f1)\n",
    "    for d in details.values():\n",
    "        ddict['tp_list'].append(d['tp'])\n",
    "        ddict['fp_list'].append(d['fp'])\n",
    "        ddict['fn_list'].append(d['fn'])\n",
    "        ddict['gold_list'].append(d.get('gold', d.get('gold_tokens', 0)))\n",
    "        ddict['pred_list'].append(d.get('pred', d.get('pred_tokens', 0)))\n",
    "\n",
    "for k in docs_gold.keys():\n",
    "    print(k)\n",
    "    gold = {\n",
    "        k: docs_gold[k]\n",
    "    }\n",
    "    pred = {\n",
    "        k: docs_pred[k]\n",
    "    }\n",
    "\n",
    "    ev = Evaluator(gold, pred)\n",
    "    ee = ev.exact()\n",
    "    print(\"    === Exact-match ===\", ee)\n",
    "    add_results(exact_results, ee)\n",
    "\n",
    "    ee = ev.substring()\n",
    "    print(\"    === Substring-match ===\", )\n",
    "    add_results(substring_results, ee)\n",
    "\n",
    "    ee = ev.iou_match(0.5)\n",
    "    print(\"    === IoU token-overlap (0.5) ===\", ee)\n",
    "    add_results(iou_results, ee)\n",
    "\n",
    "    ee = ev.token_level()\n",
    "    print(\"    === Token-level ===\", ee)\n",
    "    add_results(token_results, ee)\n",
    "    \n",
    "    print()\n",
    "\n",
    "for k, v in exact_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Exact-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in substring_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Substring-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in iou_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"IoU-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in token_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Token-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/oarga/saccrow-data/papers/benchmark_papers_all'\n",
    "\n",
    "docs_pred = {}\n",
    "docs_gold = {}\n",
    "texts = {}\n",
    "\n",
    "# list all folders in path\n",
    "import os\n",
    "import csv\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    if not os.path.isdir(os.path.join(path, f)):\n",
    "        continue\n",
    "    pred_file = 'paper/plain/processed/abstract_nofilter.csv'\n",
    "    gold_file = 'paper/plain/processed/gt_abstract.txt'\n",
    "    text_file = 'paper/plain/processed/abstract.txt'\n",
    "    pred_path = os.path.join(path, f, pred_file)\n",
    "    gold_path = os.path.join(path, f, gold_file)\n",
    "    text_path = os.path.join(path, f, text_file)\n",
    "    # read each row of txt to list\n",
    "    with open(gold_path, 'r') as inf:\n",
    "        docs_gold[f] = [line.strip() for line in inf if line.strip()]\n",
    "    # read first col of each row as csv\n",
    "    with open(pred_path, 'r') as inf:\n",
    "        reader = csv.reader(inf, delimiter=',')\n",
    "        docs_pred[f] = [row[0].strip() for row in reader if row and row[0].strip()]    \n",
    "    with open(text_path, 'r') as inf:\n",
    "        texts[f] = inf.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_results = {}\n",
    "substring_results = {}\n",
    "iou_results = {}\n",
    "token_results = {}\n",
    "\n",
    "for dd in [exact_results, substring_results, iou_results, token_results]:\n",
    "    dd['p_list'] = []\n",
    "    dd['r_list'] = []\n",
    "    dd['f1_list'] = []\n",
    "    dd['tp_list'] = []\n",
    "    dd['fp_list'] = []\n",
    "    dd['fn_list'] = []\n",
    "    dd['gold_list'] = []\n",
    "    dd['pred_list'] = []\n",
    "\n",
    "def add_results(ddict, results_item):\n",
    "    p, r, f1, details = results_item\n",
    "    ddict['p_list'].append(p)\n",
    "    ddict['r_list'].append(r)\n",
    "    ddict['f1_list'].append(f1)\n",
    "    for d in details.values():\n",
    "        ddict['tp_list'].append(d['tp'])\n",
    "        ddict['fp_list'].append(d['fp'])\n",
    "        ddict['fn_list'].append(d['fn'])\n",
    "        ddict['gold_list'].append(d.get('gold', d.get('gold_tokens', 0)))\n",
    "        ddict['pred_list'].append(d.get('pred', d.get('pred_tokens', 0)))\n",
    "\n",
    "for k in docs_gold.keys():\n",
    "    print(k)\n",
    "    gold = {\n",
    "        k: docs_gold[k]\n",
    "    }\n",
    "    pred = {\n",
    "        k: docs_pred[k]\n",
    "    }\n",
    "\n",
    "    ev = Evaluator(gold, pred)\n",
    "    ee = ev.exact()\n",
    "    print(\"    === Exact-match ===\", ee)\n",
    "    add_results(exact_results, ee)\n",
    "\n",
    "    ee = ev.substring()\n",
    "    print(\"    === Substring-match ===\", )\n",
    "    add_results(substring_results, ee)\n",
    "\n",
    "    ee = ev.iou_match(0.5)\n",
    "    print(\"    === IoU token-overlap (0.5) ===\", ee)\n",
    "    add_results(iou_results, ee)\n",
    "\n",
    "    ee = ev.token_level()\n",
    "    print(\"    === Token-level ===\", ee)\n",
    "    add_results(token_results, ee)\n",
    "    \n",
    "    print()\n",
    "\n",
    "for k, v in exact_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Exact-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in substring_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Substring-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in iou_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"IoU-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in token_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Token-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluate C/NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "# Load spaCy model for POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Candidate term extraction (noun phrases)\n",
    "# ----------------------------\n",
    "def extract_candidates(doc):\n",
    "    candidates = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        term = chunk.text.lower().strip()\n",
    "        if len(term.split()) >= 1:  # include both single and multi-word terms\n",
    "            candidates.append(term)\n",
    "    return candidates\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Compute C-value\n",
    "# ----------------------------\n",
    "def compute_c_values(corpus_candidates):\n",
    "    freq = Counter()\n",
    "    for cands in corpus_candidates:\n",
    "        freq.update(cands)\n",
    "\n",
    "    nested_map = defaultdict(list)\n",
    "\n",
    "    # Find nesting relations\n",
    "    for a in freq:\n",
    "        for b in freq:\n",
    "            if a != b and a in b:\n",
    "                nested_map[a].append(b)\n",
    "\n",
    "    c_values = {}\n",
    "    for term in freq:\n",
    "        term_len = len(term.split())\n",
    "        f_a = freq[term]\n",
    "        if term in nested_map:\n",
    "            t_a = nested_map[term]\n",
    "            nested_freq = sum(freq[b] for b in t_a) / len(t_a)\n",
    "            c_val = math.log2(term_len + 1) * (f_a - nested_freq)  # +1 to handle single words\n",
    "        else:\n",
    "            c_val = math.log2(term_len + 1) * f_a\n",
    "        c_values[term] = c_val\n",
    "    return c_values\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Compute NC-value\n",
    "# ----------------------------\n",
    "def compute_nc_values(corpus, c_values, alpha=0.8):\n",
    "    context_words = defaultdict(int)\n",
    "\n",
    "    # Collect context words from all docs\n",
    "    for doc in corpus:\n",
    "        for term in c_values:\n",
    "            if term in doc.text.lower():\n",
    "                tokens = doc.text.lower().split()\n",
    "                for i, tok in enumerate(tokens):\n",
    "                    if tok in term.split():\n",
    "                        if i > 0:\n",
    "                            context_words[tokens[i-1]] += 1\n",
    "                        if i < len(tokens)-1:\n",
    "                            context_words[tokens[i+1]] += 1\n",
    "\n",
    "    # Normalize context weights\n",
    "    total = sum(context_words.values())\n",
    "    weights = {w: c/total for w, c in context_words.items() if total > 0}\n",
    "\n",
    "    # Compute NC-values\n",
    "    nc_values = {}\n",
    "    for term, c_val in c_values.items():\n",
    "        ctx_score = 0\n",
    "        for w in term.split():\n",
    "            if w in weights:\n",
    "                ctx_score += weights[w]\n",
    "        nc_values[term] = alpha * c_val + (1-alpha) * ctx_score\n",
    "\n",
    "    return nc_values\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Ranking and Filtering\n",
    "# ----------------------------\n",
    "def rank_terms(nc_values, top_n=None, min_score=None):\n",
    "    ranked_terms = sorted(nc_values.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if min_score is not None:\n",
    "        ranked_terms = [(t, s) for t, s in ranked_terms if s >= min_score]\n",
    "\n",
    "    if top_n is not None:\n",
    "        ranked_terms = ranked_terms[:top_n]\n",
    "\n",
    "    return ranked_terms\n",
    "\n",
    "# ----------------------------\n",
    "# Example Usage with Multiple Documents\n",
    "# ----------------------------\n",
    "corpus_texts = list(texts.values())\n",
    "\n",
    "# Process corpus\n",
    "docs = [nlp(text) for text in corpus_texts]\n",
    "\n",
    "# Extract candidates from each doc\n",
    "corpus_candidates = [extract_candidates(doc) for doc in docs]\n",
    "\n",
    "# Compute C and NC values\n",
    "c_values = compute_c_values(corpus_candidates)\n",
    "nc_values = compute_nc_values(docs, c_values)\n",
    "\n",
    "# Rank terms with both options\n",
    "#print(\"Top 10 terms:\")\n",
    "#for term, score in rank_terms(nc_values, top_n=10):\n",
    "#    print(f\"{term} -> {score:.4f}\")\n",
    "\n",
    "print(\"\\nTerms with NC-value >= 1.0:\")\n",
    "selected_terms = []\n",
    "for term, score in rank_terms(nc_values, min_score=1.0):\n",
    "    print(f\"{term} -> {score:.4f}\")\n",
    "    selected_terms.append(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_term(term):\n",
    "    term = term.strip().lower()\n",
    "    if term.startswith('the '):\n",
    "        term = term[4:]\n",
    "    if term.endswith('this '):\n",
    "        term = term[:-5]\n",
    "    if term.endswith('a '):\n",
    "        term = term[:-2]\n",
    "    if term.endswith('an '):\n",
    "        term = term[:-3]\n",
    "    return term.strip()\n",
    "\n",
    "for i in range(len(selected_terms)):\n",
    "    selected_terms[i] = clean_term(selected_terms[i])\n",
    "\n",
    "docs_pred = {}\n",
    "for f in texts:\n",
    "    docs_pred[f] = set()\n",
    "    for term in selected_terms:\n",
    "        if term in texts[f].lower():\n",
    "            docs_pred[f].add(term)\n",
    "    docs_pred[f] = list(docs_pred[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_results = {}\n",
    "substring_results = {}\n",
    "iou_results = {}\n",
    "token_results = {}\n",
    "\n",
    "for dd in [exact_results, substring_results, iou_results, token_results]:\n",
    "    dd['p_list'] = []\n",
    "    dd['r_list'] = []\n",
    "    dd['f1_list'] = []\n",
    "    dd['tp_list'] = []\n",
    "    dd['fp_list'] = []\n",
    "    dd['fn_list'] = []\n",
    "    dd['gold_list'] = []\n",
    "    dd['pred_list'] = []\n",
    "\n",
    "def add_results(ddict, results_item):\n",
    "    p, r, f1, details = results_item\n",
    "    ddict['p_list'].append(p)\n",
    "    ddict['r_list'].append(r)\n",
    "    ddict['f1_list'].append(f1)\n",
    "    for d in details.values():\n",
    "        ddict['tp_list'].append(d['tp'])\n",
    "        ddict['fp_list'].append(d['fp'])\n",
    "        ddict['fn_list'].append(d['fn'])\n",
    "        ddict['gold_list'].append(d.get('gold', d.get('gold_tokens', 0)))\n",
    "        ddict['pred_list'].append(d.get('pred', d.get('pred_tokens', 0)))\n",
    "\n",
    "for k in docs_gold.keys():\n",
    "    print(k)\n",
    "    gold = {\n",
    "        k: docs_gold[k]\n",
    "    }\n",
    "    pred = {\n",
    "        k: docs_pred[k]\n",
    "    }\n",
    "\n",
    "    ev = Evaluator(gold, pred)\n",
    "    ee = ev.exact()\n",
    "    print(\"    === Exact-match ===\", ee)\n",
    "    add_results(exact_results, ee)\n",
    "\n",
    "    ee = ev.substring()\n",
    "    print(\"    === Substring-match ===\", )\n",
    "    add_results(substring_results, ee)\n",
    "\n",
    "    ee = ev.iou_match(0.5)\n",
    "    print(\"    === IoU token-overlap (0.5) ===\", ee)\n",
    "    add_results(iou_results, ee)\n",
    "\n",
    "    ee = ev.token_level()\n",
    "    print(\"    === Token-level ===\", ee)\n",
    "    add_results(token_results, ee)\n",
    "    \n",
    "    print()\n",
    "\n",
    "for k, v in exact_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Exact-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in substring_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Substring-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in iou_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"IoU-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in token_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Token-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Evaluate ChemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "def extract_keywords_from_corpus(corpus, ngram_range=(1, 3), stop_words='english', top_n=100, min_score=0.3):\n",
    "    \"\"\"\n",
    "    Extract keywords from a corpus of texts.\n",
    "    \n",
    "    Parameters:\n",
    "        corpus (list[str]): List of documents (strings).\n",
    "        ngram_range (tuple): Size of n-grams to extract (default: (1,3)).\n",
    "        stop_words (str or list): Stop words to remove (default: 'english').\n",
    "        top_n (int): Number of top candidates to return per document (default: 100).\n",
    "        min_score (float): Minimum score threshold for keeping a keyword (default: 0.3).\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: A list of unique keywords across the corpus.\n",
    "    \"\"\"\n",
    "    kw_model = KeyBERT()\n",
    "    all_keywords = set()  # use set to avoid duplicates\n",
    "    \n",
    "    for text in corpus:\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            text,\n",
    "            keyphrase_ngram_range=ngram_range,\n",
    "            stop_words=stop_words,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        # Filter by score\n",
    "        filtered = [kw for kw, score in keywords if score > min_score]\n",
    "        all_keywords.update(filtered)  # add to set\n",
    "    \n",
    "    return list(all_keywords)\n",
    "\n",
    "\n",
    "docs_pred = {}\n",
    "for k, t in texts.items():\n",
    "    docs_pred[k] = extract_keywords_from_corpus([t])\n",
    "\n",
    "docs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_results = {}\n",
    "substring_results = {}\n",
    "iou_results = {}\n",
    "token_results = {}\n",
    "\n",
    "for dd in [exact_results, substring_results, iou_results, token_results]:\n",
    "    dd['p_list'] = []\n",
    "    dd['r_list'] = []\n",
    "    dd['f1_list'] = []\n",
    "    dd['tp_list'] = []\n",
    "    dd['fp_list'] = []\n",
    "    dd['fn_list'] = []\n",
    "    dd['gold_list'] = []\n",
    "    dd['pred_list'] = []\n",
    "\n",
    "def add_results(ddict, results_item):\n",
    "    p, r, f1, details = results_item\n",
    "    ddict['p_list'].append(p)\n",
    "    ddict['r_list'].append(r)\n",
    "    ddict['f1_list'].append(f1)\n",
    "    for d in details.values():\n",
    "        ddict['tp_list'].append(d['tp'])\n",
    "        ddict['fp_list'].append(d['fp'])\n",
    "        ddict['fn_list'].append(d['fn'])\n",
    "        ddict['gold_list'].append(d.get('gold', d.get('gold_tokens', 0)))\n",
    "        ddict['pred_list'].append(d.get('pred', d.get('pred_tokens', 0)))\n",
    "\n",
    "for k in docs_gold.keys():\n",
    "    print(k)\n",
    "    gold = {\n",
    "        k: docs_gold[k]\n",
    "    }\n",
    "    pred = {\n",
    "        k: docs_pred[k]\n",
    "    }\n",
    "\n",
    "    ev = Evaluator(gold, pred)\n",
    "    ee = ev.exact()\n",
    "    print(\"    === Exact-match ===\", ee)\n",
    "    add_results(exact_results, ee)\n",
    "\n",
    "    ee = ev.substring()\n",
    "    print(\"    === Substring-match ===\", )\n",
    "    add_results(substring_results, ee)\n",
    "\n",
    "    ee = ev.iou_match(0.5)\n",
    "    print(\"    === IoU token-overlap (0.5) ===\", ee)\n",
    "    add_results(iou_results, ee)\n",
    "\n",
    "    ee = ev.token_level()\n",
    "    print(\"    === Token-level ===\", ee)\n",
    "    add_results(token_results, ee)\n",
    "    \n",
    "    print()\n",
    "\n",
    "for k, v in exact_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Exact-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in substring_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Substring-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in iou_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"IoU-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()\n",
    "\n",
    "for k, v in token_results.items():\n",
    "    import numpy as np\n",
    "    mean = np.array(v).mean()\n",
    "    std = np.array(v).std()\n",
    "    print(f\"Token-{k}: {mean:.4f} ± {std:.4f}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
